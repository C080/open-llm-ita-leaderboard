{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"E:/huggingface\"\n",
    "\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"squad_it\")\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06e4d73834d4c1285a49549e00e4b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed71cc34b4f47fabcaa57dc91cd42c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a32f8fe5c9a4b97ae4f4bcce8708a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\Samuele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987ba0da50a84b2abacc43b5c97bf72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62104abee98c4ce28dfd671221390c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/170 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samuele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Samuele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022b4638afd64704b9f8701d17ca6982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9eb1b3be5ff480bbb709a5df885014d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a7cf51b36048a2a100476eee288caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da76737de41d4708a196b08e3776b140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426b52956f1848588402300b3b9eafa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from get_model import get_model\n",
    "model_name = \"llamantino\"\n",
    "\n",
    "model, tokenizer = get_model(model_name)\n",
    "#model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_prompt import get_prompt\n",
    "\n",
    "generate_prompt, stop = get_prompt(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_question(context, question):\n",
    "    return f\"Dato il seguente testo:\\n{context}\\nRispondi brevemente a questa domanda:\\n{question}\"\n",
    "\n",
    "def build_answer(answer):\n",
    "    return f\"Risposta breve: {answer}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_shots(dataset, n):\n",
    "    conversation = []\n",
    "    for i in range(n):\n",
    "        elem = random.choice(dataset[\"train\"])\n",
    "        q_shot = build_question(elem[\"context\"], elem[\"question\"])\n",
    "        conversation.append(dict(\n",
    "            role=\"user\",\n",
    "            text=q_shot\n",
    "        ))\n",
    "        conversation.append(dict(\n",
    "            role=\"ai\",\n",
    "            text=build_answer(elem[\"answers\"][\"text\"][0])\n",
    "        ))\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def forward_model(*, prompt, model, tokenizer):\n",
    "    if \"pad_token\" not in tokenizer.special_tokens_map:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    input_ids = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "    ).input_ids\n",
    "    with torch.no_grad():\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=32,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            prompt_lookup_num_tokens=10\n",
    "        )\n",
    "\n",
    "    ret = []\n",
    "    for i in range(0, len(output_ids)):\n",
    "        generated_text = tokenizer.decode(\n",
    "            output_ids[i],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        generated_text = generated_text[len(prompt[i]):]\n",
    "\n",
    "        if \"\\n\" in generated_text:\n",
    "            generated_text = generated_text[:generated_text.index(\"\\n\")]\n",
    "        \n",
    "        ret.append(generated_text.strip())\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samuele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Samuele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.048433303833008\n",
      "['piena di sorprese e di inaspettate opportunità. Non abbiate paura di correre rischi e provare c']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "out = forward_model(\n",
    "    prompt=[\"La vita è\"],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(time.time() - t0)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7609 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7609/7609 [7:23:58<00:00,  3.50s/it]   \n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "predicted_answers = []\n",
    "theoretical_answers = []\n",
    "\n",
    "ds = dataset[\"test\"].shuffle(seed=42)\n",
    "\n",
    "bar = tqdm(ds.iter(batch_size=batch_size), total=len(ds))\n",
    "total = 0\n",
    "for elem in bar:\n",
    "    try:\n",
    "        ids = elem[\"id\"]\n",
    "        context = elem[\"context\"]\n",
    "        question = elem[\"question\"]\n",
    "        answers = elem[\"answers\"]\n",
    "\n",
    "        model_inputs = []\n",
    "        for c, q in zip(context, question):\n",
    "            model_inputs.append(\n",
    "                generate_prompt(\n",
    "                    get_shots(dataset, 3) + [\n",
    "                        dict(\n",
    "                            role=\"user\",\n",
    "                            text=build_question(c, q)\n",
    "                        )\n",
    "                    ],\n",
    "                    do_continue=True\n",
    "                ) + \" \" + build_answer(\"\").strip()\n",
    "            )\n",
    "\n",
    "        model_outputs = forward_model(\n",
    "            prompt=model_inputs,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        #print(f\"DOMANDA:{model_inputs[0][520:]}\\nRISPOSTA{model_outputs[0]}\\nSOLUZIONE{answers[0]['text']}\\n\")\n",
    "\n",
    "        for model_output, id in zip(model_outputs, ids):\n",
    "            predicted_answers.append(dict(\n",
    "                id=id,\n",
    "                prediction_text=model_output,\n",
    "            ))\n",
    "\n",
    "        for ans, id in zip(answers, ids):\n",
    "            theoretical_answers.append(dict(\n",
    "                id=id,\n",
    "                answers=ans,\n",
    "            ))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "model_name += '_13b'\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('./cache', exist_ok=True)\n",
    "with open(f\"./cache/generated-squad-{model_name}.json\", \"w\") as f:\n",
    "    json.dump(dict(\n",
    "        predicted_answers=predicted_answers,\n",
    "        theoretical_answers=theoretical_answers,\n",
    "    ), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7609\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(f\"./cache/generated-squad-{model_name}.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(len(data[\"predicted_answers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4630ddf8f2584c029577db460a5d10bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b1e23dcd4746c1b2c9d9f8c915e186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REPORT ===\n",
      "current date: 19/01/2024 03:11:59\n",
      "Dataset: SQuAD-it\n",
      "Model: llamantino_13b\n",
      "{'exact_match': 0.5388355894335655, 'f1': 11.158163481250739}\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "predicted_answers = data[\"predicted_answers\"]\n",
    "theoretical_answers = data[\"theoretical_answers\"]\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "results = metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "print(\"=== REPORT ===\")\n",
    "print(\"current date:\", time.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print(\"Dataset: SQuAD-it\")\n",
    "print(\"Model:\", model_name)\n",
    "print(results)\n",
    "print(\"==========================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
