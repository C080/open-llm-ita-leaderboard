{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"E:/huggingface\"\n",
    "\n",
    "#import transformers\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"squad_it\")\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\Samuele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86eed1dd119d43f2bac3c6ed3eaafd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from get_model import get_model\n",
    "model_name = \"maestrale\"\n",
    "\n",
    "model, tokenizer = get_model(model_name)\n",
    "#model = model.to(DEVICE) #togliere se in 4 bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_prompt import get_prompt\n",
    "# del generate_prompt\n",
    "generate_prompt, stop = get_prompt(model_name) #stop sembra useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_question(context, question):\n",
    "    return f\"Dato il seguente testo:\\n{context}\\nRispondi brevemente a questa domanda:\\n{question}\"\n",
    "\n",
    "def build_answer(answer):\n",
    "    return f\"Risposta breve: {answer}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_shots(dataset, n):\n",
    "    conversation = []\n",
    "    for i in range(n):\n",
    "        elem = random.choice(dataset[\"train\"])\n",
    "        q_shot = build_question(elem[\"context\"], elem[\"question\"])\n",
    "        conversation.append(dict(\n",
    "            role=\"user\",\n",
    "            text=q_shot\n",
    "        ))\n",
    "        conversation.append(dict(\n",
    "            role=\"assistant\",\n",
    "            text=build_answer(elem[\"answers\"][\"text\"][0])\n",
    "        ))\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Sei un assistente utile.<|im_end|>\n",
      "<|im_start|>user\n",
      "Ciao come va amico?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Tutto bene grazie.<|im_end|>\n",
      "<|im_start|>user\n",
      "E tua sorella Alice?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "è morta<|im_end|>\n",
      "<|im_start|>user\n",
      "Ah cavolo, mi dispiace.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "tranqui bro, è la vita<|im_end|>\n",
      "<|im_start|>user\n",
      "scusa come si chiama tua sorella?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "  {\"role\": \"system\", \"content\": \"Sei un assistente utile.\" },\n",
    "  {\"role\": \"user\", \"content\": \"Ciao come va amico?\" },\n",
    "  {\"role\": \"assistant\", \"content\": \"Tutto bene grazie.\" },\n",
    "  {\"role\": \"user\", \"content\": \"E tua sorella Alice?\" },\n",
    "  {\"role\": \"assistant\", \"content\": \"è morta\"},\n",
    "  {\"role\": \"user\", \"content\": \"Ah cavolo, mi dispiace.\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"tranqui bro, è la vita\"},\n",
    "  {\"role\": \"user\", \"content\": \"scusa come si chiama tua sorella?\"},\n",
    "] \n",
    "print(tokenizer.apply_chat_template(chat, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def forward_model(*, prompt, model, tokenizer):\n",
    "\n",
    "    if \"pad_token\" not in tokenizer.special_tokens_map:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    input_ids = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "    ).input_ids\n",
    "    with torch.no_grad():\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=32,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            prompt_lookup_num_tokens=10,\n",
    "            temperature = 0.7,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "        #print(output_ids)\n",
    "\n",
    "    ret = []\n",
    "    for i in range(0, len(output_ids)):\n",
    "        generated_text = tokenizer.decode(\n",
    "            output_ids[i],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        # print(generated_text)\n",
    "        # print(generated_text[len(prompt[i]):])\n",
    "        generated_text = generated_text[len(prompt[i]):] #perchè in output_ids c'è anche il prompt!\n",
    "\n",
    "        # if \"\\n\" in generated_text:\n",
    "        #     generated_text = generated_text[:generated_text.index(\"\\n\")] non necessario\n",
    "        \n",
    "        ret.append(generated_text.strip())\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.321749210357666\n",
      "['ti scrivo perché vorrei comprare un nuovo smartphone e ho già pensato a qualche modello.ho letto la t']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "out = forward_model(\n",
    "    prompt=[\"ciao amico come va?\"],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(time.time() - t0)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Sei un assistente utile.<|im_end|>\n",
      "<|im_start|>user\n",
      "Dato il seguente testo:\n",
      "La produzione di beni e servizi prodotti dal lavoro e dagli immobili situati negli Stati Uniti è diminuita a un tasso annuo di circa il 6% nel quarto trimestre del 2008 e nel primo trimestre del 2009, rispetto all' attività degli anni precedenti. Il tasso di disoccupazione negli Stati Uniti è salito al 10,1% nell' ottobre 2009, il tasso più elevato dal 1983 e circa il doppio di quello precedente alla crisi. Le ore medie per settimana lavorativa sono scese a 33, il livello più basso da quando il governo ha iniziato a raccogliere i dati nel 1964. Con il calo del prodotto interno lordo è venuto il declino dell' innovazione. Con meno risorse a rischio di distruzione creativa, il numero di domande di brevetto flat-lined. Rispetto agli ultimi 5 anni di crescita esponenziale delle domande di brevetto, questa stagnazione è correlata al calo analogo del PIL nello stesso periodo. 10.1%.\n",
      "Rispondi brevemente a questa domanda:\n",
      "Qual è stato il tasso di decremento annuo della produzione di beni e servizi prodotti dal lavoro e dall' immobiliare nel quarto trimestre 2008 e nel primo trimestre 2009?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Risposta breve: 6%<|im_end|>\n",
      "<|im_start|>user\n",
      "Dato il seguente testo:\n",
      "Una di queste reazioni alla fredda estetica del modernismo e del Brutalismo è la scuola di architettura metaforica, che comprende cose come il biomorfismo e l' architettura zoomorfa, entrambe utilizzando la natura come fonte primaria di ispirazione e design. Mentre alcuni lo considerano un aspetto del postmodernismo, altri lo considerano una scuola a sé stante e un ulteriore sviluppo dell' architettura espressionista.\n",
      "Rispondi brevemente a questa domanda:\n",
      "Quale tipo di architettura alcuni considerano l' architettura metaforica come uno sviluppo?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Risposta breve: architettura espressionista<|im_end|>\n",
      "<|im_start|>user\n",
      "Dato il seguente testo:\n",
      "La Carolina del Nord è sede di tre importanti campionati sportivi: le Carolina Panthers della National Football League e le Charlotte Hornets della National Basketball Association hanno sede a Charlotte, mentre gli uragani Carolina della Raleigh giocano nella National Hockey League. Le pantere e gli uragani sono le uniche due grandi squadre sportive professionistiche che hanno la stessa denominazione geografica giocando in aree metropolitane diverse. Gli uragani sono l' unica squadra professionistica del North Carolina ad aver vinto un campionato di serie A, dopo aver catturato la Stanley Cup nel 2006. La Carolina del Nord ospita anche Charlotte Hounds della Major League Lacrosse.\n",
      "Rispondi brevemente a questa domanda:\n",
      "Gli uragani Carolina appartengono a quale lega?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Risposta breve: National Hockey League<|im_end|>\n",
      "<|im_start|>user\n",
      "Dato il seguente testo:\n",
      "Ctenophora (ctenoforo singolare; dal greco??? kteis' pettine' e??? pher' carry', comunemente conosciuto come gelatine di pettine) è un phylum di animali che vivono in acque marine in tutto il mondo. La loro caratteristica più distintiva è il \"pettine\" - gruppi di ciglia che usano per nuotare - sono gli animali più grandi che nuotano per mezzo di ciglia. Gli adulti di varie specie variano da pochi millimetri a 1,5 m (4 piedi 11 pollici) in taglia. Come i cnidari, i loro corpi sono costituiti da una massa di gelatina, con uno strato di cellule all' esterno e un altro che riveste la cavità  interna. In ctenore, questi strati sono profondi due cellule, mentre quelli in cnidari sono profondi solo una cellula. Alcuni autori hanno combinato ctenorefori e cnidari in un unico phylum, Coelenterata, in quanto entrambi i gruppi si affidano al flusso d' acqua attraverso la cavità  corporea sia per la digestione che per la respirazione. La crescente consapevolezza delle differenze ha convinto gli autori più recenti a classificarli come phyla separati.\n",
      "Rispondi brevemente a questa domanda:\n",
      "Quali dimensioni sono adulti Ctenophora?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Risposta breve:\n"
     ]
    }
   ],
   "source": [
    "### CELLA DI PROVA DEL NUOVO GET PROMPT DI SAIGA-ITA-7B!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "predicted_answers = []\n",
    "theoretical_answers = []\n",
    "\n",
    "ds = dataset[\"test\"].shuffle(seed=42)\n",
    "\n",
    "total = 0\n",
    "for i in range(3):\n",
    "    elem     = ds[i].copy()\n",
    "    ids      = elem[\"id\"]\n",
    "    context  = elem[\"context\"]\n",
    "    question = elem[\"question\"]\n",
    "    answers  = elem[\"answers\"]\n",
    "\n",
    "    model_inputs = []\n",
    "    chat = get_shots(dataset, 3) + [\n",
    "                dict(\n",
    "                    role=\"user\",\n",
    "                    text=build_question(context, question)\n",
    "                )\n",
    "            ]\n",
    "    model_inputs.append(\n",
    "        generate_prompt(\n",
    "        # scrive tre esempi non come  prompt ma come cronologia di chat: role: user -> text: domanda, role: assistant -> text: risposta breve\n",
    "            chat,\n",
    "            do_continue=True\n",
    "        ) + \"\" + build_answer(\"\").strip()\n",
    "    )\n",
    "\n",
    "print(model_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 329/7609 [34:40<10:01:07,  4.95s/it]"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "predicted_answers = []\n",
    "theoretical_answers = []\n",
    "\n",
    "ds = dataset[\"test\"].shuffle(seed=42)\n",
    "\n",
    "bar = tqdm(ds.iter(batch_size=batch_size), total=len(ds))\n",
    "total = 0\n",
    "for elem in bar:\n",
    "    try:\n",
    "        ids = elem[\"id\"]\n",
    "        context = elem[\"context\"]\n",
    "        question = elem[\"question\"]\n",
    "        answers = elem[\"answers\"]\n",
    "\n",
    "        model_inputs = []\n",
    "        for c, q in zip(context, question):\n",
    "            model_inputs.append(\n",
    "                generate_prompt(\n",
    "                    get_shots(dataset, 3) + [\n",
    "                        dict(\n",
    "                            role=\"user\",\n",
    "                            text=build_question(c, q)\n",
    "                        )\n",
    "                    ],\n",
    "                    do_continue=True\n",
    "                ) + \" \" + build_answer(\"\").strip()\n",
    "            )\n",
    "        # print(model_inputs[0])\n",
    "        # break\n",
    "\n",
    "        model_outputs = forward_model(\n",
    "            prompt=model_inputs[0],\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        model_outputs = model_outputs[0].split(\"Risposta breve:\")[-1] if \"Risposta breve:\" in model_outputs[0] else _\n",
    "        #print(f\"DOMANDA:\\n{model_inputs[0]}\\n\\nRISPOSTA\\n{model_outputs}\\nSOLUZIONE\\n{answers[0]['text']}\\n\")#[520:]\n",
    "\n",
    "        # for model_output, id in zip(model_outputs, ids):\n",
    "        #     predicted_answers.append(dict(\n",
    "        #         id=id,\n",
    "        #         prediction_text=model_output,\n",
    "        #     ))\n",
    "        predicted_answers.append(dict(\n",
    "                id=ids[0],\n",
    "                prediction_text=model_outputs,\n",
    "            ))\n",
    "\n",
    "        for ans, id in zip(answers, ids):\n",
    "            theoretical_answers.append(dict(\n",
    "                id=id,\n",
    "                answers=ans,\n",
    "            ))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('./cache', exist_ok=True)\n",
    "with open(f\"./cache/generated-squad-{model_name}.json\", \"w\") as f:\n",
    "    json.dump(dict(\n",
    "        predicted_answers=predicted_answers,\n",
    "        theoretical_answers=theoretical_answers,\n",
    "    ), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"./cache/generated-squad-{model_name}.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(len(data[\"predicted_answers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "predicted_answers = data[\"predicted_answers\"]\n",
    "theoretical_answers = data[\"theoretical_answers\"]\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "results = metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "print(\"=== REPORT ===\")\n",
    "print(\"current date:\", time.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print(\"Dataset: SQuAD-it\")\n",
    "print(\"Model:\", model_name)\n",
    "print(results)\n",
    "print(\"==========================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
