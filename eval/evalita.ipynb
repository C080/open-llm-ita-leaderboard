{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset_name = \"irony\"\n",
    "\n",
    "assert dataset_name in [\"irony\", \"misogyny\", \"sentiment\"]\n",
    "\n",
    "if dataset_name == \"irony\":\n",
    "    fname = \"./irony.csv\"\n",
    "    dataset = pd.read_csv(fname, sep=\";\")\n",
    "    \n",
    "    dataset[\"target\"] = \"\"\n",
    "    dataset.loc[dataset[\"irony\"] == 1, \"target\"] = \"si\"\n",
    "    dataset.loc[dataset[\"irony\"] == 0, \"target\"] = \"no\"\n",
    "\n",
    "    dataset[\"text\"] = dataset[\"text\"]\n",
    "\n",
    "    classes = [\"si\", \"no\"]\n",
    "\n",
    "elif dataset_name == \"misogyny\":\n",
    "    fname = \"./misogyny.tsv\"\n",
    "    class_col = \"misogynous\"\n",
    "    dataset = pd.read_csv(fname, sep=\"\\t\")\n",
    "    \n",
    "    dataset[\"target\"] = \"\"\n",
    "    dataset.loc[dataset[\"misogynous\"] == 1, \"target\"] = \"si\"\n",
    "    dataset.loc[dataset[\"misogynous\"] == 0, \"target\"] = \"no\"\n",
    "    \n",
    "    dataset[\"text\"] = dataset[\"text\"]\n",
    "    \n",
    "    classes = [\"si\", \"no\"]\n",
    "\n",
    "elif dataset_name == \"sentiment\":\n",
    "    fname = \"./sentiment.csv\"\n",
    "    class_col = \"sentiment\"\n",
    "    dataset = pd.read_csv(fname, sep=\";\")\n",
    "    \n",
    "    dataset[\"target\"] = \"neutrale\"\n",
    "    dataset.loc[dataset[\"positivo\"] == 1, \"target\"] = \"positivo\"\n",
    "    dataset.loc[dataset[\"negativo\"] == 1, \"target\"] = \"negativo\"\n",
    "\n",
    "    dataset[\"text\"] = dataset[\"tweet\"]\n",
    "\n",
    "    classes = [\"positivo\", \"negativo\", \"neutrale\"]\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "dataset = dataset.train_test_split(test_size=0.8, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:21<00:00, 70.82s/it]\n"
     ]
    }
   ],
   "source": [
    "from get_model import get_model\n",
    "model_name = \"saiga-7b\"\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "model, tokenizer = get_model(model_name)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_prompt import get_prompt\n",
    "\n",
    "generate_prompt, stop = get_prompt(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_question(context):\n",
    "    if dataset_name == \"irony\":\n",
    "        return f\"Dato questo testo:\\n{context}\\nIl suo contenuto è ironico? (si/no)\"\n",
    "    elif dataset_name == \"misogyny\":\n",
    "        return f\"Dato questo testo:\\n{context}\\nIl suo contenuto è misogino? (si/no)\"\n",
    "    elif dataset_name == \"sentiment\":\n",
    "        return f\"Dato questo testo:\\n{context}\\nIl suo contenuto è positivo, negativo o neutrale?\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'text': 'Dato questo testo:\\n#labuonascuola di #Renzi dal pubblico al privato con 60 € almese <URL>\\nIl suo contenuto è ironico? (si/no)'}, {'role': 'ai', 'text': 'si'}, {'role': 'user', 'text': \"Dato questo testo:\\nHo capito che l'isis vuole convertire tutti all'islam però potrebbe farlo senza uccidere e obbligazioni..\\nIl suo contenuto è ironico? (si/no)\"}, {'role': 'ai', 'text': 'si'}, {'role': 'user', 'text': 'Dato questo testo:\\n<URL> anche a lui nn piace #labuonascuola #MIDAperRUOLO\\nIl suo contenuto è ironico? (si/no)'}, {'role': 'ai', 'text': 'no'}]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def get_shots(dataset, n):\n",
    "    conversation = []\n",
    "    for i in range(n):\n",
    "        elem = random.choice(dataset[\"train\"])\n",
    "        q_shot = build_question(elem[\"text\"])\n",
    "        conversation.append(dict(\n",
    "            role=\"user\",\n",
    "            text=q_shot\n",
    "        ))\n",
    "        conversation.append(dict(\n",
    "            role=\"ai\",\n",
    "            text=elem[\"target\"]\n",
    "        ))\n",
    "    return conversation\n",
    "\n",
    "shots = get_shots(dataset, 3)\n",
    "print(shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def forward_model(*, prompt, model, tokenizer):\n",
    "    if \"pad_token\" not in tokenizer.special_tokens_map:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    input_ids = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "    ).input_ids\n",
    "    with torch.no_grad():\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=32,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    ret = []\n",
    "    for i in range(0, len(output_ids)):\n",
    "        generated_text = tokenizer.decode(\n",
    "            output_ids[i],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        generated_text = generated_text[len(prompt[i]):]\n",
    "\n",
    "        if \"\\n\" in generated_text:\n",
    "            generated_text = generated_text[:generated_text.index(\"\\n\")]\n",
    "        \n",
    "        ret.append(generated_text.strip())\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 44/698 [24:32:51<364:52:09, 2008.45s/it]\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "predicted_answers = []\n",
    "theoretical_answers = []\n",
    "\n",
    "bar = tqdm(dataset[\"test\"].shuffle().iter(batch_size=batch_size), total=len(dataset[\"test\"]))\n",
    "total = 0\n",
    "for elem in bar:\n",
    "    try:\n",
    "        ids = []\n",
    "        texts = elem[\"text\"]\n",
    "        targets = elem[\"target\"]\n",
    "\n",
    "        model_inputs = []\n",
    "        for i in texts:\n",
    "            ids.append(str(uuid.uuid4()))\n",
    "            shots = get_shots(dataset, 5)\n",
    "            model_inputs.append(\n",
    "                generate_prompt(\n",
    "                    shots + [\n",
    "                        dict(\n",
    "                            role=\"user\",\n",
    "                            text=build_question(i)\n",
    "                        )\n",
    "                    ], do_continue=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        model_outputs = forward_model(\n",
    "            prompt=model_inputs,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        for model_output, id in zip(model_outputs, ids):\n",
    "            predicted_answers.append(dict(\n",
    "                id=id,\n",
    "                prediction_text=model_output,\n",
    "            ))\n",
    "\n",
    "        for o, id in zip(targets, ids):\n",
    "            theoretical_answers.append(dict(\n",
    "                id=id,\n",
    "                answer=o,\n",
    "            ))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    bar.update(len(texts))\n",
    "    total += len(texts)\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('./cache', exist_ok=True)\n",
    "\n",
    "with open(f\"./cache/generated-{dataset_name}-{model_name}.json\", \"w\") as f:\n",
    "    json.dump(dict(\n",
    "        predicted_answers=predicted_answers,\n",
    "        theoretical_answers=theoretical_answers,\n",
    "    ), f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(f\"./cache/generated-{dataset_name}-{model_name}.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(len(data[\"predicted_answers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REPORT ===\n",
      "Dataset name: irony\n",
      "Model name: saiga-7b\n",
      "Samples:  698\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          si       0.73      0.65      0.69       357\n",
      "          no       0.67      0.75      0.71       341\n",
      "\n",
      "    accuracy                           0.70       698\n",
      "   macro avg       0.70      0.70      0.70       698\n",
      "weighted avg       0.70      0.70      0.70       698\n",
      "\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "def convert_class_to_int(c):\n",
    "    c = c.lower().strip()\n",
    "    if c in classes:\n",
    "        return classes.index(c)\n",
    "    else:\n",
    "        min_dist = 100000\n",
    "        min_class = None\n",
    "        for cl in classes:\n",
    "            d = distance(c, cl)\n",
    "            if d < min_dist:\n",
    "                min_dist = d\n",
    "                min_class = cl\n",
    "        return classes.index(min_class)\n",
    "        \n",
    "\n",
    "\n",
    "for p, t in zip(data[\"predicted_answers\"], data[\"theoretical_answers\"]):\n",
    "    y_true.append(convert_class_to_int(t[\"answer\"]))\n",
    "    y_pred.append(convert_class_to_int(p[\"prediction_text\"]))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"=== REPORT ===\")\n",
    "print(\"Dataset name:\", dataset_name)\n",
    "print(\"Model name:\", model_name)\n",
    "print(\"Samples: \", len(y_true))\n",
    "print()\n",
    "print(classification_report(y_true, y_pred, target_names=classes))\n",
    "print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "results_dir = \"./results/\"\n",
    "\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "fname = f\"{results_dir}/evalita-{dataset_name}-{model_name}.json\"\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=classes, output_dict=True)\n",
    "with open(fname, \"w\") as f:\n",
    "    json.dump(report, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
